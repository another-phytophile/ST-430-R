---
title: "YuHW07ST430"
author: "Jerry Yu"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggpmisc)
library(dplyr)
library(tidyverse)
library(tinytex)
library(usethis)
library(lmtest)
library(car)
library(rsq)
library(leaps)
ftest = function(model, L, h = 0)
  # General linear test of H0: L beta = h
{
  BetaHat = model$coefficients
  dimL = dim(L)
  if (length(BetaHat) != dimL[2])
    stop("Sizes of L and Beta are incompatible")
  r = dimL[1]
  if (qr(L)$rank != r)
    stop("Rows of L must be linearly independent.")
  out = numeric(4)
  names(out) = c("F", "df1", "df2", "p-value")
  dfe = df.residual(model)
  diff = L %*% BetaHat - h
  fstat = t(diff) %*% solve(L %*% vcov(model) %*% t(L)) %*% diff / r
  # Note vcov = MSE * XtXinv
  fstat = as.numeric(fstat)
  out[1] = fstat
  out[2] = r
  out[3] = dfe
  out[4] = 1 - pf(fstat, r, dfe)
  return(out)
} 
```

```{r import1}
ucars <- read.csv("Datasets/Used_Cars.txt") %>% as_tibble()
ucars
```

```{r model1}
attach(ucars)
ucarsfm <- lm(Asking.Price~Mileage + Price.New + Avg.Retail)
summary(ucarsfm)

avis <- confint(ucarsfm, level = 0.95) %>% as.data.frame()
avis[2,2]
```
# Question 1

## a)	Interpret each of the coefficients in the final model using proper units and a suitable increment (i.e. a 1-unit increase might be too small to consider for some of the terms in your model).  Also find and discuss a 95% CI for each predictor variables. 

- At a given Mileage and a given remaining loan value, an increase of 1000 dollars of the new price of the cars results in a $`r -1*summary(ucarsfm)[[4]][2,1]` decrease in the asking price of a car. 

- The 95% CI for Mileage is (`r avis[2,1]`,`r avis[2,2]`). This means that we are 95% confident that the true mean slope of the Mileage variable is within the confidence interval. 

- At a given new price of a car and a given remaining loan value, an increase in 1000 miles driven on the car results in a $`r 1000*summary(ucarsfm)[[4]][3,1]` decrease in the asking price of a car. 

- The 95% CI for Price when New is (`r avis[3,1]`,`r avis[3,2]`). This means that we are 95% confident that the true mean slope of the Car Price When New variable is within the confidence interval. 

- At a given new price of a car and a given mileage, an increase in 100 dollars in the average retail price of the new car results in a $`r 100*summary(ucarsfm)[[4]][4,1]` decrease in the asking price of a car. 

- The 95% CI for Average Retail is (`r avis[2,1]`,`r avis[2,2]`). This means that we are 95% confident that the true mean slope of the Average NEw Retail Price variable is within the confidence interval. 

## b) Examine residual plots and a normal quantile plot of and comment on the adequacy of your model.   

```{r 2 plots1}
ucarsfmr <- tibble(
  "fit" = ucarsfm$fitted.values,
  "resid" = ucarsfm$residuals
)

ggplot(ucarsfmr,aes(x=fit,y=resid))+
  geom_jitter(color="darkgreen")+
  geom_hline(yintercept = 0, linetype="dotted")+
  labs(title = paste("Q1: Residuals Versus Fitted Values for the Ucars Data Set"),
         subtitle = "by Jerry Yu")+ 
  theme(plot.title = element_text(size = 14))

ucarsfmrs <- add_column(ucarsfmr, "rstandardized"=rstandard(ucarsfm))

ggplot(data = ucarsfmrs, aes(sample = resid))+
  geom_qq( color="coral")+
  geom_qq_line( color="turquoise")+
  labs(
    title = paste("Q1: Normal Quantile Plot of Residuals for Ucars Linear Regression Model"),
    subtitle = "by Jerry Yu"
  ) +
  xlab("Theoretical Quantiles")+
  ylab("Sample Quantiles")
```
Our Model does not seem adequate. There seems to be a slight fan shaped distribution in the patterns of the residuals on the residuals in the residual plot. This indicates potential non constant error variation which is a violation of the assumption of constant error needed for linear regression. Additionally, the extreme residuals of the residual plot does not look like the theoritical residuals (the amplitude is higher). 

## 3 )	Conduct Breusch-Pagan Test for the constancy of the error variance. Be sure to give an appropriate null and alternate hypothesis, test statistic, its associated degrees of freedom, and the p-value.


```{r bp1}
ucarsfmbp <- bptest(ucarsfm,studentize = FALSE)
ucarsfmbp 
ucarsfmbp[[4]]
```
H0: Equal Variance Among Errors (Homoscedasticity) 
HA: Unequal Variance Among Errors (Heteroscedasticity)
Statistic: `r ucarsfmbp[[1]]`
Df: `r ucarsfmbp[[2]]`
P Value: `r ucarsfmbp[[4]]`

## d)	Index Plot to test for Independence of errors and write your comments.

```{r index}
ggplot(ucarsfmr, aes(x = 1:length(resid), y = resid)) +
  geom_point(color = "aquamarine") +
  labs(x = "Index",
       title = "Q1 Residual Time Sequence Plot for Ucars Data",
       subtitle = "by Jerry Yu") +
  geom_hline(yintercept = 0,
             color = "darkblue",
             linetype = "dotdash")
```

The spread of the errors does not seem to have a pattern. Thus there is no evidence to support the claim that the erros are not independent. 

## e)	Conduct Durbin-Watson Test. Be sure to give an appropriate null and alternate hypothesis, test statistic and the p-value. 

```{r dw1}
dwtest(ucarsfm)
ucarsfmw <-durbinWatsonTest(ucarsfm)
ucarsfmw 
```

H0: Errors are independent. (autocorrelation = 0)
HA: Errors are not independent. (autocorrelation $\neq$ 0)
Statistic: `r ucarsfmw[[2]]`
P: `r ucarsfmw[[2]]`

## f)	Give a Histogram of the residuals and write your comment. 

```{r density1, warning=FALSE}
ggplot(data = ucarsfmrs, aes(x = resid)) +
  geom_histogram(fill = "palegreen") +
  labs(
    title = paste("Histogram of Residuals for Data Set UCars"),
    subtitle = "by Jerry Yu"
  ) +
  ylab("Count of Residuals")
```

There seems to be a right skew of the data, with there being more extreme values on the high side of the residuals, around 4000. This indicates that there are likley outliers. 

## g)	Conduct a Shapiro-Wilk Test on the residuals. Be sure to give an appropriate null and alternate hypothesis, test statistic and the p-value.  Give the p-value for this test and explain what this means in terms of our model assumptions.

```{r shapiro1}
shap1 <- shapiro.test(ucarsfmrs$resid)
shap1 %>% str()
shap1[[2]]
```

H0:The random error in our model is normally distributed. 
HA: The random error in our model is not normally distributed. 
Statistic: `r shap1[[1]]`
P: `r shap1[[2]]`
Conclusion: As `r shap1[[2]]` < 0.05, at $\alpha$ = 0.05 we conclude that there is evidence to support that the distribution of random error (residuals) in our model is not normal. 

## h)	Check for large leverage points and identify the row numbers.

```{r lev1}
# find critical value
crit <- 2*summary(ucarsfm)$coeff %>% nrow() / nrow(ucars)
# derive row numbers and hat values
ucarsfml <- add_column(ucars,"hatvalues"=hatvalues(ucarsfm),
                       "rownum" = rownames(ucars))
# plot
ggplot(ucarsfml, aes(x = 1:length(hatvalues), y = hatvalues)) +
  geom_point(color = "darkorange") +
  labs(x = "Hat Values",
       title = "Hatvalues Plot with Critical Value for Ucars Data",
       subtitle = "by Jerry Yu") +
  geom_hline(yintercept = crit,
             color = "purple",
             linetype = "dotdash",
             size = 1) +
  xlab("Index")
# print row numbers for large leverage points
ucarsfml %>% subset(hatvalues> crit) %>% select(c("rownum"))
```

## i)	Check for outliers and identify the row numbers

```{r out1}
# derive row numbers
ucarsfmro <- add_column(ucars,"rstandardized"=rstandard(ucarsfm),
                       "rownum" = rownames(ucars))
# plot
ggplot(ucarsfmrs, aes(x = fit, y = rstandardized)) +
  geom_point(color = "lightgreen") +
  labs(x = "Index",
       title = "Outlier Detection Plot with Standarized Residuals vs Fit",
       subtitle = "by Jerry Yu") +
  geom_hline(yintercept = 0,
             color = "darkblue",
             linetype = "solid") +
    geom_hline(yintercept = -2,
             color = "blue",
             linetype = "dashed")+
  geom_hline(yintercept = 2,
             color = "blue",
             linetype = "dashed")
# print row numbers for large leverage points
ucarsfmro %>% subset(abs(rstandardized)> 2) %>% select(c("rownum"))
```

## j)	Check for influential points and identify the row numbers

```{r inf1}
# find cutoff value
cutoff <- with(ucarsfm,4/df.residual)
# derive row numbers and hat values
ucarsfmc <- add_column(ucars,"Di"=cooks.distance(ucarsfm),
                       "rownum" = rownames(ucars))
# plot
ggplot(ucarsfmc, aes(x = 1:length(Di), y = Di)) +
  geom_point(color = "violet") +
  labs(x = "Hat Values",
       title = "Cook's Distance Plot with Critical Value for Ucars Data",
       subtitle = "by Jerry Yu") +
  geom_hline(yintercept = cutoff,
             color = "red",
             linetype = "dotdash",
             size = 1) +
  xlab("Index")
# print row numbers for large leverage points
ucarsfmc %>% subset(Di > cutoff) %>% select(c("rownum"))
```

## k)	Compute Variance inflation factors (VIF) and comment on the degree of collinearity.

```{r vif1}
vif(ucarsfm)

mean(vif(ucarsfm))
```
As none of the individual VIF values exceed 10, but the mean VIF exceeds 1, there is evidence of multicollinearity in our model, but no variable stands out as being strongly multicollinear with the other variables. 

## l)	Use your model to estimate the asking price for a car that was $10,000 when new, has 25,000 miles on it, and the average retail is $11,000.  


```{r predict1}
samp <- tibble("Mileage"=25,"Price.New" = 10000,"Avg.Retail"=11000)

predict(ucarsfm,samp)
```
Predicted Asking price is `r predict(ucarsfm,samp)`.










