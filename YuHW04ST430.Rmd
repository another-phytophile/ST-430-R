---
title: "Hw04ST430Yu"
author: "Haozhe (Jerry) Yu"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggpmisc)
library(dplyr)
library(tidyverse)
library(tinytex)
library(usethis)
library(lmtest)
library(car)
library(rsq)
library(Metrics)
ftest = function(model, L, h = 0)
  # General linear test of H0: L beta = h
{
  BetaHat = model$coefficients
  dimL = dim(L)
  if (length(BetaHat) != dimL[2])
    stop("Sizes of L and Beta are incompatible")
  r = dimL[1]
  if (qr(L)$rank != r)
    stop("Rows of L must be linearly independent.")
  out = numeric(4)
  names(out) = c("F", "df1", "df2", "p-value")
  dfe = df.residual(model)
  diff = L %*% BetaHat - h
  fstat = t(diff) %*% solve(L %*% vcov(model) %*% t(L)) %*% diff / r
  # Note vcov = MSE * XtXinv
  fstat = as.numeric(fstat)
  out[1] = fstat
  out[2] = r
  out[3] = dfe
  out[4] = 1 - pf(fstat, r, dfe)
  return(out)
} # End of function ftest
```

# Question 1

> Problem1A: For this problem, use the Commercial Property data set from KNNL Problem 6.18. The response variable is rental rates. The explanatory variables are age, operating  expenses and vaces, vacancy rates, and total square footage.


```{r import 1}
Property <- as_tibble(read_table("https://users.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/KutnerData/Chapter%20%206%20Data%20Sets/CH06PR18.txt",
            col_names = c("rrate","age","opex","vac","sqft")
            ))
```

## 1. Obtain a scatterplot matrix for all 5 variables (age, operating expenses and vaces, vacancy rates, total square  footage, and rental rates) and give your comments about linearity of variables.

```{r pplot}
pairs(Property)
```

Most of the variables seem lack linearity, except for sqft, which seems to positively linearly correlated with rental rate and operating expense, Rental Rate seems to also show a weak positive linear correlation with operating expense. 

## Find the correlation of all pairs of variables and give  your comments about the association between variables.

```{r cor}
PropertyCor <- cor(Property)
PropertyCor
```

My observations seem to be borne out, as the largest correlations (abs value) seem to be between rental rate and operating expenses (`r PropertyCor[3,1]`), rental rate and square footage ( `r PropertyCor[5,1]`), and square footage and operating expenses ( `r PropertyCor[3,5]`).

## 3) Run the multiple regressions with age, operating expenses and vaces, vacancy rates, and total square footage as the  explanatory variables and rental rates as the response variable.

```{r MLR}
Propertym <- lm(rrate~age + opex + vac + sqft,Property)
```

 
### a) Give the fitted (Estimated) regression equation.

Rental Rate ($Y_i$) = `r round(summary(Propertym)$coeff[1,1],3)`($beta_0$) + `r round(summary(Propertym)$coeff[2,1],3)`age($beta_1$)  + `r round(summary(Propertym)$coeff[3,1],3)`Operating_Expenses($beta_2$) + `r round(summary(Propertym)$coeff[4,1],3)`vac($beta_3$) + `r summary(Propertym)$coeff[5,1]`Square_Footage($beta_4$)

### b) Interpret the estimated parameters in the context of the problem

For each incremental increase in Age, holding all other variables constant, there is a `r round(summary(Propertym)$coeff[2,1],3)` decrease in rental rate.

For each incremental increase in Operating Expenses, holding all other variables constant, there is a  `r round(summary(Propertym)$coeff[3,1],3)`increase in rental rate.

For each incremental increase in vac, holding all other variables constant, there is a `r round(summary(Propertym)$coeff[4,1],3)` decrease in rental rate.

For each increase in 100,000 Square Feet, holding all other variables constant, there is a `r round(summary(Propertym)$coeff[5,1]*100000,3)` increase in rental rate.

### c) Give the value of R2 and Adj R2

$r^2$ = `r summary(Propertym)$r.sq`

$r^2_{adj}$ = `r summary(Propertym)$adj.r.sq`

### d) Give the results of the significance test for the null hypothesis that the four regression coefficients for the explanatory variables are all zero. Be sure to give the null and alternative hypotheses, the test 
statistic and its associated degrees of freedom, the  p-value, and a brief conclusion in words.

```{r ptable}
Propertytable <- data.frame("H0"=c("b1 = b2 = b3 = b4 =0",
                                "b0=0",
                                "b1-0",
                                "b2=0",
                                "b3=0",
                                "b4-0"),
                            "Test Statistic" = c (round(summary(Propertym)$fstat[1],3),
                                                round(summary(Propertym)$coeff[1,3],3),
                                                round(summary(Propertym)$coeff[2,3],3),
                                                round(summary(Propertym)$coeff[3,3],3),
                                                round(summary(Propertym)$coeff[4,3],3),
                                                round(summary(Propertym)$coeff[5,3],3)
                             
                            ),
                            "p-value" = c(1-pf(summary(Propertym)$fstat[1],summary(Propertym)$fstat[2],summary(Propertym)$fstat[3]),
                                                summary(Propertym)$coeff[1,4],
                                                summary(Propertym)$coeff[2,4],
                                                summary(Propertym)$coeff[3,4],
                                                summary(Propertym)$coeff[4,4],
                                                summary(Propertym)$coeff[5,4]),
                            "Reject H0?" = c("Yes",
                                             "Yes",
                                             "Yes",
                                             "Yes",
                                             "No",
                                             "Yes")
                            ) %>% as_tibble()

Propertytable 
```
 
Conclusion

- row1: as p<0.05, we reject H0 and conclude that there is evidence to support a linear relationship between at least one of the predictor variables and the response variable

- row2: as p<0.05, we reject H0 and conclude that there is evidence to support the claim that controlling for all other variables, the intercept is nonzero.

- row3: as p<0.05, we reject H0 and conclude that there is evidence to support the claim that controlling for all other variables, there exists a linear relationship between age and rental rate.

- row4: as p<0.05, we reject H0 and conclude that there is evidence to support the claim that controlling for all other variables, there exists a linear relationship between operating expenses and rental rate.

- row5: as p>0.05, we reject H0 and conclude that there is evidence to support the claim that controlling for all other variables, there exists a linear relationship between Vacancy and rental rate.

- row6: as p<0.05, we reject H0 and conclude that there is evidence to support the claim that controlling for all other variables, there exists a linear relationship between apartment square footage and rental rate.


### e) Use the general linear test approach to test whether  β3 and β4 (slopes for vacancy rate and footage) are  significantly different from zero (dropped simultaneously), use significance level of 0.05.

```{r GLT}
Propertymr <- lm(rrate~age + opex,data=Property)
anova(Propertymr,Propertym)
```

As p <0.05, we conclude at the level $\alpha$ = 0.05 that we can reject the null hypothesis and conclude that the is enough evidence to support the claim that there is a statistically significant difference between the 2 models and thus that $\beta_3$ ans $\beta_4$ are significantly different from 0. 

###f): Test Ho: β3=0 versus Ha: β3 ≠ 0 using General linear model approach.

```{r GLT2}
propertyi <- matrix(c(0,0,0,1,0),
                  nrow=1,
                  byrow = TRUE)
ftest(Propertym,propertyi)
```

### g) Remove any insignificant explanatory variables in the  model and refit the new regression model with only significant variables. 

```{r reduced}
step(Propertym)
Propertymrf <- lm(rrate~age+opex+sqft,data=Property)
summary(Propertymrf)
```

Using a Stepwise Regression, we see the only predictive variable we can  remove from the full model is vacancy rate. 

## 4) ** Confidence intervals for the regression coefficients for 
the final model. **

### I. Give the 90% confidence intervals for each of the regression coefficients. (Where 0.90 is the = "statement confidence coefficient".)

```{r confint}
confint(Propertymrf,level = 0.90) 
```

### II. Give Bonferroni Joint 90% confidence intervals for each of the regression coefficients. (Where  0.90 is the "family confidence coefficient".)

```{r bconfint}
confint(Propertymrf,level = (1-(.1/4)))
```

### III. We would like to obtain simultaneous interval estimates of the mean rental rates for four typical properties specified below. Obtain the 
family of estimates using a 95% family confidence coefficient.

```{r predb}
bonpred <- data.frame("age"=c(5.0,6.0,14.0,12.0),
           "opex"=c(8.25,8.50,11.50,10.25),
           "sqft"=c(250000,270000,300000,310000)
)
bonpred
predict(Propertymrf,
        bonpred,
        interval = "predict",
        level = (1 - (.05 / 3)))
```

### IV. Develop separate prediction intervals for the rental rates of these properties, using a 95% statement confidence coefficient in each case

```{r pred}
simpred <-data.frame("age"=c(4.0,6.0,12.0),
           "opex"=c(10.0,11.5,12.5),
           "sqft"=c(80000,120000,340000)
)
simpred
predict(Propertymrf,
        simpred,
        interval = "predict",
        level = 0/95
        )
```

## 5) Diagnostic Plots and Tests:

### a) Plot the residuals against the predicted (fitted) 
rental rate. Comment on the plot.

```{r fitplot1}

Propertymrff <- tibble(
  "fit" = Propertymrf$fitted.values,
  "resid" = Propertymrf$residuals
)

ggplot(Propertymrff,aes(x=fit,y=resid))+
  geom_jitter(color="salmon")+
  geom_hline(yintercept = 0, linetype="dotted")+
  labs(title = paste("Q1: Residuals Versus Fitted Values for the Property Data Set"),
         subtitle = "by Jerry Yu")+ 
  theme(plot.title = element_text(size = 14))
```

The residuals seem to be randomly distributed around 0, and the plot lacks any fan or funnel shapes. There is no evidence to support that variance is nonlinear or nonconstant. 

### b) Give a QQ-plot, boxplot and histogram of the residuals with normal curve. State your conclusions from these plots.

```{r 3graphs, warning=FALSE}
Propertymrfs <- add_column(Propertymrff, "rstandardized"=rstandard(Propertym))

ggplot(data = Propertymrfs, aes(sample = resid))+
  geom_qq( color="coral")+
  geom_qq_line( color="turquoise")+
  labs(
    title = paste("Q1: QQ Plot of Residuals for Property Linear Regression Model"),
    subtitle = "by Jerry Yu"
  ) +
  xlab("Theoretical Quantiles")+
  ylab("Sample Quantiles")

ggplot(Propertymrfs,aes(x=factor(NA,NA,NA),y=resid))+
  geom_boxplot(color="darkblue",fill="lavender",outlier.color="red")+
  labs(
    title = paste("Box Plot of Residuals for Data Set Property"),
    subtitle = "by Jerry Yu"
  ) +
  ylab("Residuals") +
  xlab("")+
  scale_x_discrete(labels=NULL)
  
  

ggplot(data = Propertymrfs, aes(x = resid)) +
  geom_histogram(fill = "lightblue", aes(y=after_stat(density))) +
  stat_function(fun = dnorm, color="darkgreen")+
  labs(
    title = paste("Histogram of Residuals for Data Set Property, \nNormal Curve Reference"),
    subtitle = "by Jerry Yu"
  ) +
  ylab("Density of Residuals")
  
```


### c) Conduct Breusch-Pagan Test for the constancy of the 
error variance.

```{r bp1}
Propertymrfbp <- bptest(Propertymrf,studentize = FALSE)
Propertymrfbp
```
As p <0.05, we conclude that at $\alpha$ = 0.05 that there is enough evidence to reject the null hypothesis and support the claim that the variances are unequal.

### d) Index Plot to test for Independence of errors. Conduct 
Durbin-Watson Test.

```{r index durbinwat}
ggplot(Propertymrff, aes(x = 1:length(resid), y = resid)) +
  geom_point(color = "aquamarine") +
  labs(x = "Index",
       title = "Residual Time Sequence Plot for Property Data",
       subtitle = "by Jerry Yu") +
  geom_hline(yintercept = 0,
             color = "darkblue",
             linetype = "dotdash")

Propertymw <-durbinWatsonTest(Propertymrf)
Propertymw
```
I used the `car` test where the alternative hypothesis is 2 sided. As p < 0.05, we conclude that at $\alpha$ = 0.05 that there is enough evidence to reject the null hypothesis and support the claim that true autocorrelation is not equal to zero.

### e) Conduct a Shapiro-Wilk Test on the residuals. Give 
the p-value for this test and explain what this means 
in terms of our model assumptions.

```{r shapiro1}
shap1 <- shapiro.test(Propertymrfs$resid)
shap1
```

- H0: The random error is normally distributed

- Ha: The random error is not normally distributed 

- Test Statistic: `r shap1$statistic`

- p value: `r shap1$p.value`

As p > 0.05, we fail to reject H0 at $\alpha$ = 0.05 and conclude that there is no statistically significant evidence that the random error is not normally distributed. 

### f) Deduct any outliers.

```{r outid}
Propertymro <- filter(Propertymrfs,abs(rstandardized) >2)
Propertymro
```


done in Q6

## 6) Remove the outliers and refit the model to see how much difference in R2 and adj R2

```{r woutliers}
Propertyhmerge <- bind_cols(Property,Propertymrfs)
Propertywo <- filter(Propertyhmerge,abs(rstandardized) <=2)
Propertywom <- lm(rrate~age+opex+sqft,data=Propertywo)

sumtable <- tibble("Name"=c("R^2","Adj R^2"),
                    "Full Model" = c(summary(Propertym)$r.sq,summary(Propertym)$adj.r.sq),
                    "Reduced Model" = c(summary(Propertymrf)$r.sq,summary(Propertymrf)$adj.r.sq),
                    "Reduced Model \nwout Outliers" = c(summary(Propertywom)$r.sq,summary(Propertywom)$adj.r.sq),
                   "Reduced MOdel Difference" = c(summary(Propertymrf)$r.sq-summary(Propertywom)$r.sq, summary(Propertymrf)$adj.r.sq-summary(Propertywom)$adj.r.sq)
                   )

sumtable
```

When we remove outliers, the R squared and Adjusted R squared values increase. 

# Problem1B:

## 1) Write the regression equation in matrix form

Y = X$\beta$ + $\epsilon$

## i) what are the dimenions of X, (row, column) form.

(77,4)

## ii) what are the dimesions of $\beta$ hat = b? (row, column) form.

(4,1)

## iii) What are the dimensions of $\epsilon$ hat = e ? The answer is a pair 
of numbers, number of rows and number of columns.

(77,1)

iv. What are the dimensions of e'e?

(1,1)

v. What are the dimensions of the y hat matrix? The answer is a pair of numbers, number of rows and number of columns.

(77,1)

vi. What are the dimensions of the hat matrix H? The answer is a pair of numbers, number of rows and number  of columns.

(77,77)

vii. What is e'e?

```{r mresid}
e <- as.matrix(Propertywom$resid)

t(e)%*%e
```
## 2. Calculate: Let your Xh values be the average of all predictor variables.

```{r mcalc}
y <- as.matrix(Propertywo[,1])
x <- Propertywo[,c(2,3,5)] %>% mutate("int" =1) %>% select(int,everything()) %>%  as.matrix()
```

betahat

```{r mcalc1}
solve(t(x)%*%x)%*%t(x)%*%y
```
variance

```{r mvar}
mse <- (t(e)%*%e)[1]
mse*solve(t(x)%*%x)
```